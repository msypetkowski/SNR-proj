\documentclass[a4paper]{article}

\usepackage[a4paper,  margin=1.0in]{geometry}

\usepackage{graphicx}
\usepackage{float}
\usepackage{hyperref}


\usepackage[utf8]{inputenc}
\begin{document}


\title{SNR classes project - birds species recognition using deep neural networks
- first stage report}

\author{Micha≈Ç Sypetkowski, Marcin Lew, Filip Smurawa}
\maketitle

\section{General information}
Git repository \url{https://github.com/msypetkowski/SNR-proj.git}.
We use the following tools:
\begin{itemize}
    \item \textbf{Python}\footnote{\url{https://www.python.org/}}
    \item \textbf{OpenCV}\footnote{\url{https://opencv.org/}}
    \item \textbf{NumPy}\footnote{\url{http://www.numpy.org/}}
    \item \textbf{TensorFlow}\footnote{\url{https://www.tensorflow.org/ }}
\end{itemize}
We also use Tensorboard for model and training progress visualization.
The project is tested to run in Linux environment.

\section{Multilayer Perceptron}

\subsection{Data}
Dataset consist of 50 subsets -- types of bird species.
Each subset is a set of 60 different pictures.
Altogether it gives us a data set of 3000 pictures.

We divided this dataset, so that there are 300 examples in test set and
2700 examples in training set.
Number of examples per each class is equal in both training and test set
(54 examples per class in training set and 6 in test set).
Additionally, training set is augmented during training (see section \ref{augmentation}).


\subsection{Data augmentation}
\label{augmentation}
Every training image is randomly rotated, flipped or cropped.
Augmentation is done online, so the number of training examples is
roughly infinite.
We skip the distributions of the transformations in this document.
Instead we attach an example visualization.
Raw -- unaugmented example is shown on in figure \ref{fig:aug1}.
Corresponding augmented examples are shown in figure \ref{fig:aug2}.

\begin{figure}[h]
    \caption[]{Not augmented example}
    \centering
    \includegraphics[page=2,width=0.1\textwidth]{aug1.png}
    \label{fig:aug1}
\end{figure}

\begin{figure}[h]
    \caption[]{Augmented examples corresponding to raw example in figure \ref{fig:aug1}}
    \centering
    \includegraphics[page=2,width=1.0\textwidth]{aug2.png}
    \label{fig:aug2}
\end{figure}


\subsection{HOG features}
To get HOG features, we first resize an image to 128x64x3.
Then we calculate the features using OpenCV library:
\begin{verbatim}
    hog = cv2.HOGDescriptor()
    fd = hog.compute(img).flatten()
\end{verbatim}
Image is first divided into 8x8 cells (in result, we get 16 * 8 cells).
We use gradient orientation bins of size 9.
Then the algorithm takes 16x16 blocks (4 cells) --
there are 7 * 15 such blocks.
Histograms of each of 4 cells in block are concatenated an L2 normalized.
In the end all blocks histograms are concatenated giving
final feature vecor of length (7 * 15) * 36 = 3780.
We directly feed such vectors into our network.

\subsection{Model architecture}
We selected the architecture by experimenting.
Our network has 3 hidden dense layers of sizes 256, 128 and 64.
Since we have small dataset, larger models tended overfitting
(despite of augmentation).
Each of these layers have corresponding batch normalization layer.
After the last layer there is a softmax function.
We use cross entropy as a loss function.
We tested 2 models with different activation functions:
\begin{itemize}
    \item relu
    \item sigmoid
\end{itemize}
Visualization of the model is shown in figure \ref{fig:arch}.

% TODO: draw different - more simple schema instead of tensorboard printscreen
\begin{figure}[H]
    \caption[]{Visualization of multilayer perceptron architecture (relu activation variant)}
    \centering
    \includegraphics[page=1,width=1.0\textwidth]{architecture.pdf}
    \label{fig:arch}
\end{figure}

\subsection{Training and results}
Accuracy and loss curves on training and validation set are shown in figure \ref{fig:training}.
Training and validation curves diverge from the begining because of small dataset (one epoch is around 20 iterations).
Because of batch normalization, validation accuracy increases along with training accuracy (but slower).
Full results on testset for the model using relu are shown in figure \ref{fig:eval}.
Final accuracy of relu model is around 24\%.
Sigmoid model achieves slightly lower accuracy around 21\% (nothing unusual here).

% TODO: hide the fact that this is tensorboard printscreen or do other - external visualization
\begin{figure}[H]
    \caption[]{Accuracy and loss curves}
    \centering
    \includegraphics[page=2,width=1.0\textwidth]{training.png}
    \label{fig:training}
\end{figure}

\begin{figure}[H]
    \caption[]{Results on testset (blue - good answers)}
    \centering
    \includegraphics[page=2,width=1.0\textwidth]{eval.png}
    \label{fig:eval}
\end{figure}

\newpage
\section{Convolutional neural networks}

\subsection{Ground truth model}

We trained for 1500 iterations 8-th layer of VGG16\footnote{\url{https://arxiv.org/abs/1409.1556}}
pretrained on ImageNet\footnote{\url{https://arxiv.org/abs/1409.0575}} dataset.
Then we finetuned it for 1000 iterations achieving final accuracy of 67\%.
Training and validation curves are shown in figure \ref{fig:trainingVGG}.
Full results on testset shown in figure \ref{fig:evalVGG}.
The ckpt file is downloaded from \url{http://download.tensorflow.org/models/vgg_16_2016_08_28.tar.gz}.

\begin{figure}[h]
    \caption[]{Accuracy and loss curves of VGG16 pretrained model}
    \centering
    \includegraphics[page=2,width=1.0\textwidth]{vggTraining.png}
    \label{fig:trainingVGG}
\end{figure}

\begin{figure}[h]
    \caption[]{Results on testset of VGG16 finetuned model (blue - good answers)}
    \centering
    \includegraphics[page=2,width=1.0\textwidth]{evalVGG.png}
    \label{fig:evalVGG}
\end{figure}


% Since we have small dataset, we are not sure if a very deep architecture
% will be efficient in our case.
% Eventually, we will consider architectures designed
% for more shallow models (e.g. VGG \footnote{\url{https://arxiv.org/abs/1409.1556}}).
% We are planning to use {ResNet}\footnote{\url{https://arxiv.org/abs/1512.03385}}
% as a baseline model.



\end{document}
