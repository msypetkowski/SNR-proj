\documentclass[a4paper]{article}

\usepackage[a4paper,  margin=1.0in]{geometry}

\usepackage{graphicx}
\usepackage{float}
\usepackage{hyperref}


\usepackage[utf8]{inputenc}
\begin{document}


\title{SNR classes project - birds species recognition using deep neural networks}

\author{Micha≈Ç Sypetkowski, Marcin Lew, Filip Smurawa}
\maketitle

\section{General information}
Git repository \url{https://github.com/msypetkowski/SNR-proj.git}.
We use the following tools:
\begin{itemize}
    \item \textbf{Python}\footnote{\url{https://www.python.org/}}
    \item \textbf{OpenCV}\footnote{\url{https://opencv.org/}}
    \item \textbf{NumPy}\footnote{\url{http://www.numpy.org/}}
    \item \textbf{TensorFlow}\footnote{\url{https://www.tensorflow.org/ }}
\end{itemize}
We also use Tensorboard for model and training progress visualization.
The project is tested to run in Linux environment.

\section{Multilayer Perceptron}

\subsection{Data}
The data set consist of 50 subsets -- types of bird species.
Each subset is a set of 60 different pictures.
Altogether it gives us a data set of 3000 pictures.

We divided this dataset, so that there are 300 examples in test set and
2700 examples in training set.
Number of examples per each class is equal in both training and test set
(54 examples per class in training set and 6 in test set).
Additionally, training set is augmented during training (see section \ref{augmentation}).


\subsection{Data augmentation}
\label{augmentation}
Every training image is randomly rotated, flipped or cropped.
Augmentation is done online, so the number of training examples is
practically infinite.
We skip the distributions of the transformations in this document.
Instead we attach an example visualization.
Raw -- unaugmented example is shown on on figure \ref{fig:aug1}.
Corresponding augmented examples are shown on figure \ref{fig:aug2}.

\begin{figure}[h]
    \caption[]{Not augmented example}
    \centering
    \includegraphics[page=2,width=0.1\textwidth]{aug1.png}
    \label{fig:aug1}
\end{figure}

\begin{figure}[h]
    \caption[]{Augmented examples corresponding to raw example on figure \ref{fig:aug1}}
    \centering
    \includegraphics[page=2,width=1.0\textwidth]{aug2.png}
    \label{fig:aug2}
\end{figure}


\subsection{HOG features}
To get HOG features, we first resize an image to 128x64x3.
Then we calculate the features using OpenCV library:
\begin{verbatim}
    hog = cv2.HOGDescriptor()
    fd = hog.compute(img).flatten()
\end{verbatim}
Image is first divided into 8x8 cells (in result, we get 16 * 8 cells).
We use gradient orientation bins of size 9.
Then the algorithm takes 16x16 blocks (4 cells) --
there are 7 * 15 such blocks.
Histograms of each of 4 cells in block are concatenated an L2 normalized.
In the end all blocks histograms are concatenated giving
final feature vecor of length (7 * 15) * 36 = 3780.
We directly feed such vectors into our network.

\subsection{Model architecture}
We selected the architecture by experimenting.
Our network has 3 hidden dense layers of sizes 256, 128 and 64.
Larger models tended overfit since we have small dataset
(despite of augmentation).
Each of these layers have corresponding batch normalization layer.
After the last layer there is a softmax function.
We use cross enthropy as a loss function.
We tested 2 models with different activation functions:
\begin{itemize}
    \item relu
    \item sigmoid
\end{itemize}
Visualization of the model is shown on figure \ref{fig:arch}.

% TODO: draw different - more simple schema instead of tensorboard printscreen
\begin{figure}[h]
    \caption[]{Tensorboard visualization of multilayer perceptron architecture (relu activation variant)}
    \centering
    \includegraphics[page=2,width=1.0\textwidth]{architecture.png}
    \label{fig:arch}
\end{figure}

\subsection{Training and results}
Accuracy and loss curves on training and validation set are shown on figure \ref{fig:training}.
Full results on testset for the model using relu are shown on figure \ref{fig:eval}.
Final accuracy of relu model is around 24\%.

\begin{figure}[h]
    \caption[]{Results on testset (blue - good answers)}
    \centering
    \includegraphics[page=2,width=1.0\textwidth]{eval.png}
    \label{fig:eval}
\end{figure}

% TODO: hide the fact that this is tensorboard printscreen or do other - external visualization
\begin{figure}[h]
    \caption[]{Accuracy and loss curves}
    \centering
    \includegraphics[page=2,width=1.0\textwidth]{training.png}
    \label{fig:training}
\end{figure}

\section{Convolutional neural networks}
Not done yet.

\section{SVM}
Not done yet.



\end{document}
